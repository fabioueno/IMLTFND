{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine\n",
    "\n",
    "Support Vector Machine (SVM) is an algorithm for splitting data like the Perceptron, but finding the line as far away from the points as possible.\n",
    "\n",
    "<br><img src=\"images/support-vector-machine.png\" width=\"720px\">\n",
    "\n",
    "SVMs are algorithms that attempt to classify data, i.e. find a boundary that splits the data, but contains a margin as wide as possible, penalizing not only misclassified points, but those in the margin as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Error\n",
    "\n",
    "<br>\n",
    "<img src='images/classification-error-split.png' style='float: left' width='480px'>\n",
    "<img src='images/classification-error-unified.png' style='float: right' width='480px'>\n",
    "<div style='clear: both'></div>\n",
    "<br>\n",
    "\n",
    "As we can see, to penalize points in the margin, we use these boundaries as starting points for measuring the error. Red points above the lower boundary and blue points below the upper boundary are classified as incorrect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Margin Error\n",
    "\n",
    "Since we want the largest margin, we're going to assign an error for each margin like so:\n",
    "\n",
    "* Large margin has smaller error.\n",
    "* Small margin has larger error.\n",
    "\n",
    "The margin is simply:\n",
    "\n",
    "$$\n",
    "Margin = \\frac{2}{|W|}\n",
    "$$\n",
    "\n",
    "And the error can be easily calculated as:\n",
    "\n",
    "$$\n",
    "Error = |W|^2\n",
    "$$\n",
    "\n",
    "<br><img src='images/margin-error.png' width='720px'><br>\n",
    "\n",
    "Let's look at an example:\n",
    "\n",
    "<br>\n",
    "<img src='images/margin-error-example-1.png' style='float: left' width='480px'>\n",
    "<img src='images/margin-error-example-2.png' style='float: right' width='480px'>\n",
    "<div style='clear: both'></div>\n",
    "<br>\n",
    "\n",
    "Notice the second equation is the first multiplied by 2, which plots the same line.\n",
    "\n",
    "As we can see, larger values of $|W|$ yields larger error, just as we wanted.\n",
    "\n",
    "<br><img src='images/margin-error-example-summary.png' width='720px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C Parameter\n",
    "\n",
    "The C parameter is just a constant that is attached to the classification error and its value change the focus of our SVM:\n",
    "\n",
    "* If C is large, then we're focusing on correctly classifying the points, and the tradeoff is that it may find a small margin.\n",
    "* If C is small, then we're focusing on finding a large margin, and the tradeoff is that it may make classification errors.\n",
    "\n",
    "<br><img src='images/c-parameter.png' width='720px'><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Kernel\n",
    "\n",
    "Sometimes we have a dataset that's easy to split with a line, like the following:\n",
    "\n",
    "<br>\n",
    "<img src='images/kernel-line-1.png' style='float: left' width='480px'>\n",
    "<img src='images/kernel-line-2.png' style='float: right' width='480px'>\n",
    "<div style='clear: both'></div>\n",
    "<br>\n",
    "\n",
    "But there are times when it's not enough:\n",
    "\n",
    "<br><img src='images/kernel-parabola-1.png' width='720px'><br>\n",
    "\n",
    "When this is the case, what we do is add another dimension and use the points in a plane. Now we can, for instance, use a parabola with equation $y = x^2$ and plot the points accordingly:\n",
    "\n",
    "<br><img src='images/kernel-parabola-2.png' width='720px'><br>\n",
    "\n",
    "Now we have a line that can easily separate the points. When the bring them back to the $x$ axis, we have two lines:\n",
    "\n",
    "<br><img src='images/kernel-parabola-3.png' width='720px'><br>\n",
    "\n",
    "This is known as the Kernel Trick.\n",
    "\n",
    "We can even go further, when we already have 2 dimensions:\n",
    "\n",
    "<br><img src='images/kernel-conic-example-1.png' width='720px'><br>\n",
    "\n",
    "In this case, we sacrify linearity and use a higher degree polynomial:\n",
    "\n",
    "<br><img src='images/kernel-conic-example-2.png' width='720px'><br>\n",
    "\n",
    "When we use the equation $x^2 + y^2$, the blue points result in 2 and the red points result in 18. The point in the middle is 10, so we use the equation $x^2 + y^2 = 10$ to separate them.\n",
    "\n",
    "<br><img src='images/kernel-conic-example-3.png' width='720px'><br>\n",
    "\n",
    "In summary, the the Kernel Trick takes the points in a given dimensions and send it to a higher dimension space. Then, we find an equation that separates the points and project back to the original dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RBF Kernel\n",
    "\n",
    "RBF Kernel is a trick that uses the Radial Basis Function (RBF) to separate the points. To do that, we draw a \"mountain\" and transfer the points to this mountain. Then, we look for a line that suits our needs and the intersection should be the boundaries:\n",
    "\n",
    "<br>\n",
    "<img src='images/rbf-kernel-1.png' style='float: left' width='480px'>\n",
    "<img src='images/rbf-kernel-2.png' style='float: right' width='480px'>\n",
    "<div style='clear: both'></div>\n",
    "<br>\n",
    "\n",
    "To build these mountains, we use the RBF on each point, multiply by some weight (scalar) the sum:\n",
    "\n",
    "<br>\n",
    "<img src='images/rbf-kernel-3.png' style='float: left' width='480px'>\n",
    "<img src='images/rbf-kernel-4.png' style='float: right' width='480px'>\n",
    "<div style='clear: both'></div>\n",
    "<br>\n",
    "\n",
    "And to find these weights, we plot the in a graph the points where the coordinates are the values of each RBF in each point:\n",
    "\n",
    "<br><img src='images/rbf-kernel-values.png' width='720px'><br>\n",
    "\n",
    "Finally, we plot the plane (in case of a 3-dimensional problem) and the constants are the weights:\n",
    "\n",
    "<br><img src='images/rbf-kernel-graph.png' width='720px'><br>\n",
    "\n",
    "### γ Parameter\n",
    "\n",
    "The γ parameter is a tunnable hyperparameter that tells how \"pointy\" the \"mountains\" are:\n",
    "\n",
    "<br><img src='images/gamma-parameter.png' width='720px'><br>\n",
    "\n",
    "It's important to notice that the more pointy, the more it overfits, and the less pointy, the more it underfits:\n",
    "\n",
    "<br><img src='images/gamma-outcome.png' width='720px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quizes\n",
    "\n",
    "01. [Support Vector Machine](../../quizes/support-vector-machine/support-vector-machine.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
