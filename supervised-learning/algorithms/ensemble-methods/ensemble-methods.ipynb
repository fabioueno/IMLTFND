{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods\n",
    "\n",
    "In this module, we'll learn how to take a bunch of model and join them into a better model. As an example, we'll think about a test we have to answer and friends that will help us.\n",
    "\n",
    "* Bagging (Bootstrap Aggregating): In this method, each of our friend answer the whole test and we combine the results.\n",
    "* Boosting: The idea of Boosting is that our friends will each answer only parts of the test, then we combine the results into a single model.\n",
    "\n",
    "<br><img src='images/bagging.png' width='720px'><br>\n",
    "\n",
    "<img src='images/boosting-weak-learners.png' style='float: left' width='480px'>\n",
    "<img src='images/boosting-strong-learner.png' style='float: right' width='480px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- This cell was copied as is! -->\n",
    "\n",
    "## Ensembles\n",
    "\n",
    "This whole lesson (on ensembles) is about how we can combine (or ensemble) the models you have already seen in a way that makes the combination of these models better at predicting than the individual models.\n",
    "\n",
    "Commonly the \"weak\" learners you use are decision trees. In fact the default for most ensemble methods is a decision tree in sklearn. However, you can change this value to any of the models you have seen so far.\n",
    "\n",
    "### Why Would We Want to Ensemble Learners Together?\n",
    "\n",
    "There are two competing variables in finding a well fitting machine learning model: **Bias** and **Variance**. It is common in interviews for you to be asked about this topic and how it pertains to different modeling techniques. As a first pass, [the wikipedia is quite useful](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff). However, I will give you my perspective and examples:\n",
    "\n",
    "**Bias**: When a model has high bias, this means that means it doesn't do a good job of bending to the data. An example of an algorithm that usually has high bias is linear regression. Even with completely different datasets, we end up with the same line fit to the data. When models have high bias, this is bad.\n",
    "\n",
    "<img src='images/bias.svg' width='720px'>\n",
    "\n",
    "**Variance**: When a model has high variance, this means that it changes drastically to meet the needs of every point in our dataset. Linear models like the one above has low variance, but high bias. An example of an algorithm that tends to have high variance and low bias is a decision tree (especially decision trees with no early stopping parameters). A decision tree, as a high variance algorithm, will attempt to split every point into its own branch if possible. This is a trait of high variance, low bias algorithms - they are extremely flexible to fit exactly whatever data they see.\n",
    "\n",
    "<br><img src='images/variance.png' width='720px'><br>\n",
    "\n",
    "By combining algorithms, we can often build models that perform better by meeting in the middle in terms of bias and variance. There are some other tactics that are used to combine algorithms in ways that help them perform better as well. These ideas are based on minimizing bias and variance based on mathematical theories, like the central limit theorem.\n",
    "\n",
    "### Introducing Randomness Into Ensembles\n",
    "\n",
    "Another method that is used to improve ensemble methods is to introduce randomness into high variance algorithms before they are ensembled together. The introduction of randomness combats the tendency of these algorithms to overfit (or fit directly to the data available). There are two main ways that randomness is introduced:\n",
    "\n",
    "1. **Bootstrap the data** - that is, sampling the data with replacement and fitting your algorithm to the sampled data.\n",
    "\n",
    "2. **Subset the features** - in each split of a decision tree or with each algorithm used in an ensemble, only a subset of the total possible features are used.\n",
    "\n",
    "In fact, these are the two random components used in the next algorithm you are going to see called **random forests**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "[Decision Trees](../decision-trees/decision-trees.ipynb) tend to overfit, and in order to prevent this, we can can use Random Forests, which builds Decision Trees with random columns. Then, when we have a new input, we test it against all Decision Trees created and combine the results.\n",
    "\n",
    "<br><img src='images/random-forests.png' width='720px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging\n",
    "\n",
    "For simplicity, we'll say that our Weak Learners are One-Node Decision Trees, so each splits the data in two:\n",
    "\n",
    "<br><img src='images/bagging-one-node.png' width='720px'><br>\n",
    "\n",
    "What we do is that random samples of the data (note that since it's random, some points could be picked more than once, and some might not be picked at all) and run our Decision Tree:\n",
    "\n",
    "<br>\n",
    "<div style='display: flex'>\n",
    "    <img src='images/bagging-decision-trees-1.png' style='margin-top: 0' width='240px'>\n",
    "    <img src='images/bagging-decision-trees-2.png' style='margin-top: 0' width='240px'>\n",
    "    <img src='images/bagging-decision-trees-3.png' style='margin-top: 0' width='240px'>\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "Then we combine them:\n",
    "\n",
    "<br><img src='images/bagging-decision-trees-result.png' width='720px'><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "\n",
    "For the Boosting algorithm, we start feeding our first learner which will try to maximize accuracy. Then, the misclassified points get a larger weight and the second learner focuses on those. And we repeat the process. Finally, we combine them:\n",
    "\n",
    "<br>\n",
    "<div style='display: flex'>\n",
    "    <img src='images/adaboost-1.png' style='margin-top: 0' width='240px'>\n",
    "    <img src='images/adaboost-2.png' style='margin-top: 0' width='240px'>\n",
    "    <img src='images/adaboost-3.png' style='margin-top: 0' width='240px'>\n",
    "</div>\n",
    "<br>\n",
    "<img src='images/adaboost-result.png' width='720px'>\n",
    "<br>\n",
    "\n",
    "Now, let's do some Math! The first thing, will be assigning a weight to each data point and calculating the correct and the wrong ones. With that done, we increase the weight of the incorrect ones until the values are the same:\n",
    "\n",
    "<br>\n",
    "<img src='images/adaboost-calculation-1.1.png' style='float: left' width='480px'>\n",
    "<img src='images/adaboost-calculation-1.2.png' style='float: right' width='480px'>\n",
    "<div style='clear: both'></div>\n",
    "<br>\n",
    "\n",
    "At this point, our first learner can't develop any further, so we do the same with the second, i.e. split the data, calculate the weights, update them and move on:\n",
    "\n",
    "<br>\n",
    "<img src='images/adaboost-calculation-2.1.png' style='float: left' width='480px'>\n",
    "<img src='images/adaboost-calculation-2.2.png' style='float: right' width='480px'>\n",
    "<div style='clear: both'></div>\n",
    "<br>\n",
    "\n",
    "When we're done with the second learner, we move the third, repeat the process, until we're satisfied:\n",
    "\n",
    "<br><img src='images/adaboost-calculation-3.1.png' width='720px'><br>\n",
    "\n",
    "Just like weighting the points for the models, we can weight the models itself - to future combine them -, assigning large positive values for models that perform well, large negative values for models that perform poorly, and values near zero for models that correcly classifies 50% of the points.\n",
    "\n",
    "<br>\n",
    "<img src='images/adaboost-model-weights.png' style='float: left' width='480px'>\n",
    "<img src='images/adaboost-model-ln.png' style='float: right' width='480px'>\n",
    "<div style='clear: both'></div>\n",
    "<br>\n",
    "\n",
    "Now, to combine, we use the weights in each area, sum the values, and classify: positive values yeild blue areas, negative values yeild red areas:\n",
    "\n",
    "<br>\n",
    "<img src='images/adaboost-example-model-weights.png' style='float: left' width='480px'>\n",
    "<img src='images/adaboost-example-weights-sum.png' style='float: right' width='480px'>\n",
    "<div style='clear: both'></div>\n",
    "<br>\n",
    "\n",
    "<img src='images/adaboost-example-result.png' width='720px'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quizes\n",
    "\n",
    "01. [Adaboost](../../quizes/adaboost/adaboost.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
