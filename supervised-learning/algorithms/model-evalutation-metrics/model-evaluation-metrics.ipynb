{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation Metrics\n",
    "\n",
    "When we're developing our algorithms, it's important to know how to evaluate and improve our models based on some metrics and be careful not to overfit.\n",
    "\n",
    "<br>\n",
    "<img src='images/overfit-regression.png' style='float: left' width='480px'>\n",
    "<img src='images/overfit-classification.png' style='float: right' width='480px'>\n",
    "<div style='clear: both'></div>\n",
    "<br>\n",
    "\n",
    "As we can see above, we split the data into two: the training set and the test set, and their names are self-explanatory - we use the former to train and the latter to test after training. It's important to not use the testing set to train!\n",
    "\n",
    "<br><img src='images/split-data-rule.png' width='720px'><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "The Confusion Matrix is used to check the rights and wrongs of our model's predictions.\n",
    "\n",
    "<br>\n",
    "<img src='images/confusion-matrix-medical-data.png' style='float: left' width='480px'>\n",
    "<img src='images/confusion-matrix-email-data.png' style='float: right' width='480px'>\n",
    "<div style='clear: both'></div>\n",
    "\n",
    "### Type 1 and Type 2 Errors\n",
    "\n",
    "Sometimes in the literature, you'll see False Positives and False Negatives as Type 1 and Type 2 errors. Here is the correspondence:\n",
    "\n",
    "* Type 1 Error (Error of the first kind, or False Positive): In the medical example, this is when we misdiagnose a healthy patient as sick.\n",
    "* Type 2 Error (Error of the second kind, or False Negative): In the medical example, this is when we misdiagnose a sick patient as healthy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "\n",
    "Accuracy is one of the metrics used to evaluate our model. It answers the following question: \"Out of all the data, how many points did we classify correctly?\"\n",
    "\n",
    "<br><img src='images/accuracy.png' width='720px'><br>\n",
    "\n",
    "Sometimes the accuracy is not good enough:\n",
    "\n",
    "<br><img src='images/accuracy-problem.png' width='720px'>\n",
    "\n",
    "*Note: The denominator should be 284887 instead of 284807.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision\n",
    "\n",
    "Precision answers the following question: \"Out of all the points predicted to be positive, how many of them were actually positive?\". Models interested in precision try to minimize the false positive values.\n",
    "\n",
    "<br>\n",
    "<img src='images/precision-medical-data.png' style='float: left' width='480px'>\n",
    "<img src='images/precision-email-data.png' style='float: right' width='480px'>\n",
    "<div style='clear: both'></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall\n",
    "\n",
    "Recall answers the following question: \"Out of points that are labeled 'positive', how many of them were correctly predicted as positve?\". Models interested in recall try to minimize the false negative values.\n",
    "\n",
    "<br>\n",
    "<img src='images/recall-medical-data.png' style='float: left' width='480px'>\n",
    "<img src='images/recall-email-data.png' style='float: right' width='480px'>\n",
    "<div style='clear: both'></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F1\n",
    "\n",
    "F1 is a metrics that looks Precision and Recall at the same time, using the Harmonic Mean:\n",
    "\n",
    "$$\n",
    "F_1 Score = \\frac{2 \\times Precision \\times Recall}{Precision + Recall}\n",
    "$$\n",
    "\n",
    "## Fβ\n",
    "\n",
    "The Fβ is a generalization of The F1 score, where we can increase the imporance of a metric - Precision or Recall - as we'd like and the formula is the following:\n",
    "\n",
    "$$\n",
    "F_\\beta = (1 + \\beta^2) \\times \\frac{Precision \\times Recall}{\\beta^2 \\times Precision + Recall}\n",
    "$$\n",
    "\n",
    "The minimum value of β is 0 and it results in:\n",
    "\n",
    "$$\n",
    "F_0 = (1 + 0^2) \\times \\frac{Precision \\times Recall}{0^2 \\times Precision + Recall} = \\frac{Precision \\times Recall}{Recall} = Precision\n",
    "$$\n",
    "\n",
    "So lower values of β increase the weight of Precision.\n",
    "\n",
    "After making a quick refactoring, we can compute the opposite:\n",
    "\n",
    "$$\n",
    "F_\\beta = (1 + \\beta^2) \\times \\frac{Precision \\times Recall}{\\beta^2 \\times Precision + Recall} = \\frac{Precision \\times Recall}{\\frac{\\beta^2}{1 + \\beta^2} \\times Precision + \\frac{1}{1 + \\beta^2} \\times Recall}\n",
    "$$\n",
    "\n",
    "Towards infinity:\n",
    "$$\n",
    "\\lim_{n \\to \\infty} F_n = \\frac{Precision \\times Recall}{1 \\times Precision + 0 \\times Recall} = \\frac{Precision \\times Recall}{Precision} = Recall\n",
    "$$\n",
    "\n",
    "So higher values of β increase the weight of Recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Receiver Operator Characteristic (ROC)\n",
    "\n",
    "The ROC Curve works on top of two rates:\n",
    "\n",
    "* True Positive Rate\n",
    "* False Positive Rate\n",
    "\n",
    "<br><img src='images/roc-curve.png' width='720px'><br>\n",
    "\n",
    "We do this by splitting the data in different points and plotting the graph. The idea is that the nature of the data is related to the graph (or the area under it):\n",
    "\n",
    "* Random split has an area approximately equal to 0.5.\n",
    "* A good split has an area around 0.8.\n",
    "* A perfect split has an area of 1.0.\n",
    "\n",
    "<br><img src='images/roc-areas.png' width='720px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Metrics\n",
    "\n",
    "There are a few metric functions we can use that are very easy to compute with Scikit-learn:\n",
    "\n",
    "<br>\n",
    "<img src='images/mean-absolute-error-sklearn.png' style='float: left' width='480'>\n",
    "<img src='images/mean-squared-error-sklearn.png' style='float: right' width='480px'>\n",
    "<div style='clear: both'></div>\n",
    "<br>\n",
    "<img src='images/r2-score-sklearn.png' width='720px'>\n",
    "<br>\n",
    "\n",
    "* Mean Absolute Error (MAE) is equal to the sum of the absolute value of the distances of the points to the line.\n",
    "* Mean Squared Error (MSE) is equal to the sum of the squared value of the distances of the points to the line.\n",
    "* R2 Score is equal to $1 - \\frac{MSE}{MSE_{avg}}$, where $MSE_{avg}$ is the MSE calculated on the horizontal line where $y$ is the average of all points.\n",
    "\n",
    "<br><img src='images/r2-score.png' width='720px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quizes\n",
    "\n",
    "01. [Split Data](../../quizes/split-data/split-data.ipynb)\n",
    "02. [Classification Metrics](../../quizes/classification-metrics/classification-metrics.ipynb)\n",
    "03. [Regression Metrics](../../quizes/regression-metrics/regression-metrics.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
