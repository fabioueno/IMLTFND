{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "Decision Trees works by narrowing down options by asking questions.\n",
    "\n",
    "<br>\n",
    "<img src=\"images/recommending-apps.png\" style=\"float: left\" width=\"480px\">\n",
    "<img src=\"images/students-admission.png\" style=\"float: right\" width=\"480px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy\n",
    "\n",
    "The entropy is a physics concept and measure how much liberty a particle has.\n",
    "\n",
    "<br><img src=\"images/entropy.png\" style=\"float: left; margin-right: 40px\" width=\"480px\">\n",
    "\n",
    "Using the three forms of water:\n",
    "\n",
    "* Solid: Low entropy, particles can't move much.\n",
    "* Liquid: Medium entropy, particles can move a little.\n",
    "* Gas: High entropy, particles can move freely.\n",
    "\n",
    "<div style=\"clear: both\"></div>\n",
    "<br>\n",
    "\n",
    "This notion of entropy can also be used in probability:\n",
    "\n",
    "<br>\n",
    "<img src=\"images/entropy-example-1.png\" style=\"float: left\" width=\"480px\">\n",
    "<img src=\"images/entropy-example-2.png\" style=\"float: right\" width=\"480px\">\n",
    "<div style=\"clear: both\"></div>\n",
    "<br>\n",
    "\n",
    "And the general formula:\n",
    "\n",
    "<br><img src=\"images/entropy-formula.png\" width=\"720px\"><br>\n",
    "\n",
    "*Note: Where the denominator is $m - n$ should be $m + n$.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Class Entropy\n",
    "\n",
    "The formula above can be extended for multiple classes:\n",
    "\n",
    "$$\n",
    "entropy = - p_1 \\log_2(p_1) - p_2 \\log_2(p_2) - ... - p_n \\log_2(p_n) = - \\sum_{i=1}^{n} p_i \\log_2(p_i)\n",
    "$$\n",
    "\n",
    "Where $p_n$ is the probability of $n$ and is expressed as:\n",
    "\n",
    "$$\n",
    "p_n = \\frac{m_n}{m_1 + m_2 + ... + m_n}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Gain\n",
    "\n",
    "Information Gain is just change in entropy and can be calculated by subtracting the weighted average of children's entropy from the parent's entropy.\n",
    "\n",
    "<br><img src=\"images/information-gain.png\" width=\"720px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "It's possible to tune a number of parameters of Decision Trees, such as the minimum number os samples to split and the minimum number of samples in each leaf. Although this is tunable, it's important to be careful not to underfit or overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quizes\n",
    "\n",
    "01. [Information Gain](../../quizes/information-gain/information-gain.ipynb)\n",
    "02. [Decision Trees](../../quizes/decision-trees/decision-tress.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
