{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "Decision Trees works narrowing down options by asking questions.\n",
    "\n",
    "<br><img src='assets/images/decision-trees.png' alt='Decision Trees' style='width: 720px'><br>\n",
    "\n",
    "The example of the students admission used in the introduction of Supervised Learning could also use Decision Trees:\n",
    "\n",
    "<br><img src='assets/images/decision-trees-students.png' alt='Decision Trees Students' style='width: 720px'><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy\n",
    "\n",
    "The entropy is a physics concept and measure how much liberty a particle has.\n",
    "\n",
    "<br><img src='assets/images/entropy.png' alt='Entropy' style='width: 720px'><br>\n",
    "\n",
    "Using the three forms of water:\n",
    "\n",
    "* Solid: Low entropy, particles can't move much.\n",
    "* Liquid: Medium entropy, particles can move a little.\n",
    "* Gas: High entropy, particles can move freely.\n",
    "\n",
    "This notion of entropy can also be used in probability:\n",
    "\n",
    "<br><img src='assets/images/entropy-balls.png' alt='Entropy Balls' style='width: 720px'><br>\n",
    "\n",
    "The higher the entropy, the lower knowledge we have. In the example, we know exactly what'll be the color of a ball picked at random. In the second, we have a small chance of guessing. In the last, it's 50/50.\n",
    "\n",
    "When calculating the probability of independent events, say getting a specific sequence of balls with repetition, we multiply the probabilities of each:\n",
    "\n",
    "$$\n",
    "\\frac{3}{4} \\times \\frac{3}{4} \\times \\frac{3}{4} \\times \\frac{1}{4} = 0.75 \\times 0.75 \\times 0.75 \\times 0.25 = 0.105\n",
    "$$\n",
    "\n",
    "If we imagine thousands of events, between 0 and 1, it gets complicated. For this reason, we use logarithm:\n",
    "\n",
    "<br><img src='assets/images/probability-entropy.png' alt='Probability Entropy' style='width: 720px'><br>\n",
    "\n",
    "For another case:\n",
    "\n",
    "<br><img src='assets/images/probability-entropy-example.png' alt='Probability Entropy Example' style='width: 720px'><br>\n",
    "\n",
    "And the general formula:\n",
    "\n",
    "<br><img src='assets/images/probability-entropy-formula.png' alt='Probability Entropy Formula' style='width: 720px'><br>\n",
    "\n",
    "*Note: Where the denominator is $m - n$ should be $m + n$.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Class Entropy\n",
    "\n",
    "The formula above can be extended for multiple classes:\n",
    "\n",
    "$$\n",
    "entropy = - p_1 \\log_2(p_1) - p_2 \\log_2(p_2) - ... - p_n \\log_2(p_n) = - \\sum_{i=1}^{n} p_i \\log_2(p_i)\n",
    "$$\n",
    "\n",
    "Where $p_n$ is the probability of $n$ and is expressed as:\n",
    "\n",
    "$$\n",
    "p_n = \\frac{m_n}{m_1 + m_2 + ... + m_n}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Gain\n",
    "\n",
    "Information Gain is just change in entropy and can be calculated by subtracting the weighted average of children's entropy from the parent's entropy.\n",
    "\n",
    "<br><img src='assets/images/information-gain.png' alt='Information Gain' style='width: 720px'><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "It's possible to tune a number of parameters of Decision Trees, such as the minimum number os samples to split and the minimum number of samples in each leaf. Although this is tunable, it's important to be careful not to underfit or overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quizes\n",
    "\n",
    "01. [Information Gain](quizes/information-gain/information-gain.ipynb)\n",
    "02. [Decision Trees](quizes/decision-trees/decision-tress.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
